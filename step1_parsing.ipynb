{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-mining of the Tweets\n",
    "We have to collect data from July 13, 2009 to September 16, 2009 (announcement of elections, end of the drama). For the sake of monitoring the sentiment more safely, we will take +- 7 days to each date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "el_announcement_2009 = \"2009-07-06\" # The date for the 2009 Japanese elections was first announced on July 13, 2009, by then Prime Minister Taro Aso. He decided to dissolve the House of Representatives and called for a general election.\n",
    "el_elected_2009 = \"2009-09-23\" # The drama surrounding the Japanese elections of 2009 can be considered to have ended by September 16, 2009, when the newly elected Prime Minister Yukio Hatoyama was officially appointed by Emperor Akihito and his cabinet was announced. This marked the conclusion of the election process and the beginning of the new government."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.twitter import get_tweets_between_dates\n",
    "import pandas as pd\n",
    "\n",
    "# Loading the mapping file and keywords that we will be using to filter and collect the tweets.\n",
    "mapping_df = pd.read_excel('./src/mappings/mapping_elections_jap.xlsx')\n",
    "Party_JPN = mapping_df['Party_JPN'].to_list()\n",
    "Chairman_JPN = mapping_df['Chairman_JPN'].to_list()\n",
    "TAGS = ['ç·é¸æŒ™'] + Party_JPN + Chairman_JPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Searching for ç·é¸æŒ™ from 2009-07-06 to 2009-07-07 (Attempt 1). Length: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=%E7%B7%8F%E9%81%B8%E6%8C%99+lang%3Aja+since%3A2009-07-06+until%3A2009-07-07+&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe: blocked (404)\n",
      "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=%E7%B7%8F%E9%81%B8%E6%8C%99+lang%3Aja+since%3A2009-07-06+until%3A2009-07-07+&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.\n",
      "Errors: blocked (404), blocked (404), blocked (404), blocked (404)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ ERROR: 4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=%E7%B7%8F%E9%81%B8%E6%8C%99+lang%3Aja+since%3A2009-07-06+until%3A2009-07-07+&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.\n",
      " ç·é¸æŒ™ lang:ja since:2009-07-06 until:2009-07-07 \n",
      "ðŸ”Ž Searching for ç·é¸æŒ™ from 2009-07-07 to 2009-07-08 (Attempt 1). Length: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Collecting maximum 3000 tweets per day. It will take a while to process (110 minutes).\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[39m=\u001b[39m get_tweets_between_dates(TAGS, el_announcement_2009, el_elected_2009) \n\u001b[0;32m      3\u001b[0m df\u001b[39m.\u001b[39mto_excel(\u001b[39m'\u001b[39m\u001b[39m./src/data/2009_elections.xlsx\u001b[39m\u001b[39m'\u001b[39m, engine\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mxlsxwriter\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\amadeus\\Desktop\\2023-hse-diploma\\core\\twitter.py:71\u001b[0m, in \u001b[0;36mget_tweets_between_dates\u001b[1;34m(tags, start_date, end_date, limit)\u001b[0m\n\u001b[0;32m     68\u001b[0m overall_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n\u001b[0;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m tags:\n\u001b[1;32m---> 71\u001b[0m     tag_df \u001b[39m=\u001b[39m get_tweets_for_tag(tag, start_date, end_date, limit\u001b[39m=\u001b[39;49mlimit)\n\u001b[0;32m     72\u001b[0m     overall_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([overall_df, tag_df])\n\u001b[0;32m     74\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\amadeus\\Desktop\\2023-hse-diploma\\core\\twitter.py:51\u001b[0m, in \u001b[0;36mget_tweets_for_tag\u001b[1;34m(tag, start_date, end_date, max_retries, limit)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mðŸ”Ž Searching for \u001b[39m\u001b[39m{\u001b[39;00mtag\u001b[39m}\u001b[39;00m\u001b[39m from \u001b[39m\u001b[39m{\u001b[39;00mstart_str\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00mend_str\u001b[39m}\u001b[39;00m\u001b[39m (Attempt \u001b[39m\u001b[39m{\u001b[39;00mretries\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m). Length: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(tag_df)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m     tag_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([tag_df, get_tweets(tag, \u001b[39m'\u001b[39;49m\u001b[39mja\u001b[39;49m\u001b[39m'\u001b[39;49m, start_str, end_str, limit, exclude_retweets\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)])\n\u001b[0;32m     52\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\amadeus\\Desktop\\2023-hse-diploma\\core\\twitter.py:18\u001b[0m, in \u001b[0;36mget_tweets\u001b[1;34m(query, lang_code, start_date, end_date, max_tweets, exclude_retweets)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mif\u001b[39;00m exclude_retweets:\n\u001b[0;32m     16\u001b[0m     search_query \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m-filter:nativeretweets -filter:retweets\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfor\u001b[39;00m i, tweet \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sntwitter\u001b[39m.\u001b[39mTwitterSearchScraper(search_query)\u001b[39m.\u001b[39mget_items()):\n\u001b[0;32m     19\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m max_tweets:\n\u001b[0;32m     20\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amadeus\\Desktop\\2023-hse-diploma\\venv\\lib\\site-packages\\snscrape\\modules\\twitter.py:1661\u001b[0m, in \u001b[0;36mTwitterSearchScraper.get_items\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1658\u001b[0m params \u001b[39m=\u001b[39m paginationParams\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m   1659\u001b[0m \u001b[39mdel\u001b[39;00m params[\u001b[39m'\u001b[39m\u001b[39mcursor\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m-> 1661\u001b[0m \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_api_data(\u001b[39m'\u001b[39m\u001b[39mhttps://api.twitter.com/2/search/adaptive.json\u001b[39m\u001b[39m'\u001b[39m, _TwitterAPIType\u001b[39m.\u001b[39mV2, params, paginationParams, cursor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cursor):\n\u001b[0;32m   1662\u001b[0m \t\u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_v2_timeline_instructions_to_tweets_or_users(obj)\n",
      "File \u001b[1;32mc:\\Users\\amadeus\\Desktop\\2023-hse-diploma\\venv\\lib\\site-packages\\snscrape\\modules\\twitter.py:761\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._iter_api_data\u001b[1;34m(self, endpoint, apiType, params, paginationParams, cursor, direction)\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    760\u001b[0m \t_logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRetrieving scroll page \u001b[39m\u001b[39m{\u001b[39;00mcursor\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 761\u001b[0m \tobj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_api_data(endpoint, apiType, reqParams)\n\u001b[0;32m    762\u001b[0m \t\u001b[39myield\u001b[39;00m obj\n\u001b[0;32m    764\u001b[0m \t\u001b[39m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amadeus\\Desktop\\2023-hse-diploma\\venv\\lib\\site-packages\\snscrape\\modules\\twitter.py:727\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._get_api_data\u001b[1;34m(self, endpoint, apiType, params)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[39mif\u001b[39;00m apiType \u001b[39mis\u001b[39;00m _TwitterAPIType\u001b[39m.\u001b[39mGRAPHQL:\n\u001b[0;32m    726\u001b[0m \tparams \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mparse\u001b[39m.\u001b[39murlencode({k: json\u001b[39m.\u001b[39mdumps(v, separators \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m params\u001b[39m.\u001b[39mitems()}, quote_via \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mparse\u001b[39m.\u001b[39mquote)\n\u001b[1;32m--> 727\u001b[0m r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get(endpoint, params \u001b[39m=\u001b[39;49m params, headers \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apiHeaders, responseOkCallback \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_api_response)\n\u001b[0;32m    728\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    729\u001b[0m \tobj \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\amadeus\\Desktop\\2023-hse-diploma\\venv\\lib\\site-packages\\snscrape\\base.py:251\u001b[0m, in \u001b[0;36mScraper._get\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 251\u001b[0m \t\u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(\u001b[39m'\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\amadeus\\Desktop\\2023-hse-diploma\\venv\\lib\\site-packages\\snscrape\\base.py:242\u001b[0m, in \u001b[0;36mScraper._request\u001b[1;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[0;32m    240\u001b[0m \t\tsleepTime \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mattempt \u001b[39m# exponential backoff: sleep 1 second after first attempt, 2 after second, 4 after third, etc.\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \t\t_logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWaiting \u001b[39m\u001b[39m{\u001b[39;00msleepTime\u001b[39m:\u001b[39;00m\u001b[39m.0f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 242\u001b[0m \t\ttime\u001b[39m.\u001b[39;49msleep(sleepTime)\n\u001b[0;32m    243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m \tmsg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retries\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m requests to \u001b[39m\u001b[39m{\u001b[39;00mreq\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m failed, giving up.\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Collecting maximum 3000 tweets per day. It will take a while to process (110 minutes).\n",
    "# If 404 error, please use another VPN. It means that your IP has been banned from Twitter.\n",
    "df = get_tweets_between_dates(TAGS, el_announcement_2009, el_elected_2009) \n",
    "df.to_excel('./src/data/2009_elections.xlsx', engine='xlsxwriter', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Merging existing .xlsx data to get the one large dataset\n",
    "This is purely for me, just because I've often wrongly collected data on different time periods, however this is still data that could be processed. Purpose of this notebook is to merge files, sort by datetime, drop duplicate tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from core.utils import convert_to_datetime\n",
    "\n",
    "# This will take a while. It is not recommended to launch this on weak machines. It will require decent RAM and CPU power.\n",
    "df_old = pd.read_excel('./src/data/raw/2009_elections_old.xlsx')\n",
    "df_new = pd.read_excel('./src/data/raw/2009_elections_new.xlsx').drop(columns=['Unnamed: 0'])\n",
    "old_columns = df_old.columns.to_list()\n",
    "new_columns = df_old.columns.to_list()\n",
    "\n",
    "print(f'Old columns: {len(old_columns)} | New columns: {len(new_columns)}')\n",
    "print(f'Old: {len(df_old)} | New: {len(df_new)}')\n",
    "# We will check if the columns are the same.\n",
    "if old_columns == new_columns:\n",
    "    print(f'âœ… Columns are the same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case, we will convert the dates to datetime objects.\n",
    "df_old = convert_to_datetime(df_old)\n",
    "df_new = convert_to_datetime(df_new)\n",
    "# count NaN values in tweet_url\n",
    "print(f'Old: {df_old[\"tweet_url\"].isna().sum()} | New: {df_new[\"tweet_url\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes\n",
    "df_merged = pd.concat([df_old, df_new])\n",
    "print(f'Old: {len(df_old)} | New: {len(df_new)} | Merged: {len(df_merged)}')\n",
    "# Sort by datetime\n",
    "df_merged = df_merged.sort_values('tweet_date')\n",
    "\n",
    "# Drop duplicate rows by tweet_id\n",
    "df_merged = df_merged.drop_duplicates(subset=['tweet_conversationId', 'tweet_id', 'tweet_rawContent'])\n",
    "print(f'Old: {len(df_old)} | New: {len(df_new)} | Merged: {len(df_merged)}')\n",
    "df_merged.to_excel('./src/data/2009_elections.xlsx', index=False, encoding='utf-8', engine='xlsxwriter')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
